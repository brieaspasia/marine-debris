---
title: "BarChart-Debris"
author: "Brie Sherow"
date: "04/08/2020"
output: html_document
  html_document:
    toc: yes
    toc_float: yes
    code_folding: hide
    df_print: paged
  pdf_document:
    toc: yes
---

```{r setup, include=FALSE, results='hide'}
knitr::opts_chunk$set(echo = TRUE)
```

```{r load-libraries, echo=TRUE}
library(ggplot2) #graphing
library(ggthemes) #graphing templates
library(hrbrthemes) #graphing templates
library(lubridate) #date manipulation
library(forcats) #working with factors
library(tidyverse) #manipulating data
library(knitr) #rmarkdown functions
library(kableExtra) #table layouts
library(magick) #image processing
library(stats) #R stats functions
library(broom) #create summaries from stats objects
library(car) #lm regression
library(MASS) #glmm stats (.nb)
library(lme4) #glmm (glmer)
library(DHARMa) #testing model diagnostics
library(glmmTMB) #fit zero-inflated negative binomial
library(mvabund) #for multivariate stats
```

```{r create-site-select}
#read in processed survey data
data <- read.csv(file="data/all_data.csv", 
                 header=T, sep=",", 
                 fileEncoding="UTF-8-BOM") #removes special characters

data$Date <- dmy(data$Date) 
data$Notes <- as.character(data$Notes)

#keep only unsw surveys and sites with public piers and >2 survey rounds
site_select <- data %>%
  filter(Team=="UNSW" & 
           Location %in% c("DOUBL","WATSO","PARSL","NEUTR","CLIFT")) %>%
  dplyr::select(-Habitat2, -Fragment, -Whole)

#create a df for survey count per site
sampling_dates <- data %>%
  group_by(Location) %>%
  filter(Team=="UNSW") %>%
  count(Date)

#take away n
sampling_dates$n <- NULL

#create a column for survey count per site
sampling_dates <- sampling_dates %>%
  count(Location) %>%
  arrange(desc(n)) %>%
  rename(NumberSurveys=n)

#join survey count to site select
site_select <- left_join(site_select, sampling_dates, by="Location")

site_select <- site_select %>%
  mutate(survey_event = paste(Date, Site,sep=" "))

site_select %>%
  group_by(Habitat) %>%
  summarise(sum=sum(Total))
```

```{r load-item-code}
#load item names
item_code <- read.csv(file="data/CSIRO-code.csv", header=T, sep=",", fileEncoding="UTF-8-BOM") 

#filter out material as some item types may have more than one material type 
#(ex. fishing items, furniture)
item_code <- item_code %>%
  dplyr::select(ID, item_name) 
```

```{r load-site-attributes}
#load site coords and distance from harbour mouth
site_attr <- read.csv(file="data/site_attributes.csv", header=T, sep=",", fileEncoding="UTF-8-BOM") 
```


```{r item-counts-mean}
#find the mean count of each item type per site
site_total_item <- site_select %>%
  group_by(Site, Material, ID) %>% #looking at item types per site
  summarise(sum=sum(Total), #total number of each item per site
            mean=sum/unique(NumberSurveys), #mean of each item per site
            sd=sd(Total)) %>% #sd of total
            arrange(desc(mean)) #listed by largest means

site_total_item <- left_join(site_total_item, item_code, by="ID") #adding item name
  
write.csv(site_total_item, "output/total_item.csv", row.names = FALSE) #save as csv file
```


```{r top-items}
#Find the top item types across all surveys
total_item <- site_select %>%
  group_by(Material, ID) %>%
  mutate(sum=sum(Total), #sum of total items by type
         sd=sd(Total)) %>% #sd of total items by type
        dplyr::select(ID, Material, sum, sd) %>% #remove irrelevant columns
        filter(sum>0) %>% #remove zero values
        arrange(desc(sum)) %>% #show high counts first
        distinct() %>% #remove duplicate entries
        ungroup()

total_item <- left_join(total_item, item_code, by="ID") #join item names

top_items <- total_item %>% top_n(20, sum) #show top 20 items total

top_items <- top_items %>%
           mutate(pct=sum/sum(sum)*100)

top20items <- top_items %>%
  mutate_if(is.numeric, format, digits=2) %>%
  kable("html", 
      col.names = c("ID", "Material", "Sum", "SD", "Name", "Percent"),
      caption = "Top 20 items across all surveys") %>% 
  kable_styling(bootstrap_options="condensed", position="left")
top20items #print table
  
save_kable(top20items, "figures/top20items.png") #save table as png
``` 


```{r material-counts}
# rename material categories 
site_total_mat <- site_select

#state variables to rename
var_change <- c("Ceramic", "Undetermined", "E-Waste", "Paper", "Construction", "Organic", "Unknown", "Timber", "Foam", "Brick or Cement", "Rubber", "Cloth", "") 

#change Material to character
site_total_mat$Material <- as.character(site_total_mat$Material) 

#set lesser used categories to 'Other'
site_total_mat$Material[site_total_mat$Material %in% var_change] = "Other" 
  
#find the top distribution of material type across all sites
total_mat <- site_total_mat %>%
  group_by(Site, Material) %>%  #group by material findings at each site
  summarise(sum=sum(Total), #total items per each material per site
            mean=mean(sum)/unique(NumberSurveys), #mean of items by material type per site
            sd=sd(Total)) %>% 
            ungroup()

#create df for labeling habitats
site_habitat <- site_select %>%
  dplyr::select(Site, Habitat, Location) %>%
  distinct() 

#join for labeling
total_mat <- left_join(total_mat, site_habitat, by="Site") 

LocationLabs <- c(NEUTR = "Neutral Bay", CLIFT = "Clifton Gardens", PARSL = "Parsley Bay", WATSO = "Watsons Bay", DOUBL = "Double Bay")
HabitatLabs <- c(P = "Pier", SS = "Sediment")

p_total_mat <- total_mat %>%
  ggplot( aes(x=Habitat, y=mean, fill=Material)) +
  geom_bar(position="dodge", stat="identity") +
  geom_errorbar(aes(ymin=mean-sd, ymax=mean+sd, width=0.2), position=position_dodge(width=0.90)) +
  theme(axis.text.x=element_text(color = "black", size=11, 
                                 vjust=.8, angle=30, hjust=0.8)) +
  ylab("Mean item count by material type") + 
  scale_x_discrete(labels= HabitatLabs) +
  ggtitle("Mean of items by material per site") +
  facet_wrap(~Location, labeller = labeller(Location=LocationLabs), scales = "free_y")

p_total_mat

ggsave(filename = "figures/total_mat.png", plot = p_total_mat, dpi = 600)
``` 
```{r pier-vs-ss-items}
#isolate Pier sites
here::here()
total_hab <- site_select

Location = unique(total_hab$Location) %>% as.character()
      
for(x in Location){
  
#2.1 Define temp Location
  total_hab <- total_hab %>% filter(Location==x) %>%
  group_by(Habitat) %>% #group by site
  summarise(sum=sum(Total), #total items per site
            mean=mean(sum)/unique(NumberSurveys), #mean of items by hab
            sd=sd(Total)) %>% 
            ungroup()

#2.11 Material by land use
p_hab <- ggplot(total_hab, aes(x = Habitat, y = mean, fill = Habitat)) +
         geom_bar(position="dodge", stat="identity") +
  geom_errorbar(aes(ymin=mean-sd, ymax=mean+sd, width=0.2), position=position_dodge(width=0.90)) +
         geom_text(aes(label = mean), vjust = -0.5, color = "black", size = 2) +
         theme(axis.text.x=element_text(color = "black", vjust=.8,
                                 angle=30, hjust=0.8)) +
         ggtitle("Mean debris count by habitat", subtitle = x) +
         labs(y="Mean debris count per habitat", x="Habitat")

 ggsave(filename = paste0(here::here("Output"),"/",x,"_hab_mean.jpg"),
        plot = p_hab, dpi = 600)

dev.off()

}

```
 
```{r pier-items}
#isolate Pier sites
total_P <- total_mat %>%
  filter(Habitat=="P")

p_total_P <- total_P %>%
  ggplot( aes(x=Material, y=mean, fill=Material)) +
  geom_bar(position="dodge", stat="identity") +
  geom_errorbar(aes(ymin=mean-sd, ymax=mean+sd, width=0.2), position=position_dodge(width=0.90)) +
  theme(axis.text.x=element_text(color = "black", vjust=.8,
                                 angle=30, hjust=0.8)) +
  ylab("Mean item count by material type") +
  ggtitle("Mean of items by material at pier sites") +
  facet_wrap(~Location, labeller=labeller(Location=LocationLabs), scales = "free_y")

p_total_P

ggsave(filename = "figures/total_P.png", plot = p_total_P, dpi = 600)
```


```{r sediment-items}
#isolate Sediment sites
total_SS <- total_mat %>%
  filter(Habitat=="SS")

p_total_SS <- total_SS %>%
  ggplot( aes(x=Material, y=mean, fill=Material)) +
  geom_bar(position="dodge", stat="identity") +
  geom_errorbar(aes(ymin=mean-sd, ymax=mean+sd, width=0.2), position=position_dodge(width=0.90)) +
  theme(axis.text.x=element_text(color = "black", vjust=.8, 
                                 angle=30, hjust=0.8)) +
  ylab("Mean item count by material type") + 
  ggtitle("Mean of items by material at sediment sites") +
  facet_wrap(~Location, 
             labeller=labeller(Location=LocationLabs), 
             scales = "free_y")

p_total_SS

ggsave(filename = "figures/total_SS.png", plot = p_total_P, dpi = 600)
```
```{r top-items-per-habitat}
total_per_habitat_site <- site_select %>%
  group_by(Habitat, Location) %>% #separate the items by habitat type
  mutate(mean=sum(Total)/NumberSurveys, #sum of total items by type
         sd=sd(Total)) %>% #sd of total items by type
        dplyr::select(Habitat, Location, mean, sd) %>% #remove irrelevant columns
        arrange(desc(mean)) %>% #show high counts first
        distinct() %>% #remove duplicate entries
        ungroup()

write.csv(total_per_habitat_site, "output/habitat_site.csv", row.names = FALSE) #save this data frame as a csv file

p_habitat <- total_per_habitat_site %>%
  ggplot( aes(x=Habitat, y=mean, fill=Habitat)) +
  geom_bar(position="dodge", stat="identity") +
  geom_errorbar(aes(ymin=mean-sd, ymax=mean+sd, width=0.2), position=position_dodge(width=0.90)) +
  facet_wrap(~Location, scales="free_y")

p_habitat

#Is the top items list different by habitat than the overall top items in chunk 7?
item_total_per_habitat <- site_select %>%
  group_by(Habitat, ID) %>% #separate the items by habitat type
  mutate(sum=sum(Total), #sum of total items by type
         sd=sd(Total)) %>% #sd of total items by type
        dplyr::select(ID, Habitat, sum, sd)%>% #remove irrelevant columns
        filter(sum>0) %>% #remove null values
        arrange(desc(sum)) %>% #show high counts first
        distinct() %>% #remove duplicate entries
        ungroup()

#join item names
item_total_per_habitat <- left_join(item_total_per_habitat, item_code, by="ID") 

top_items_per_pier <- item_total_per_habitat %>% 
  filter(Habitat=="P") %>%
  top_n(10, sum) #show top 10 items over all piers

top_items_per_ss <- item_total_per_habitat %>% 
  filter(Habitat=="SS") %>%
  top_n(10, sum) #show top 10 items over all sediment sites

#combine the two tables
top_items_per_habitat <- rbind(top_items_per_pier, top_items_per_ss)

#create kable with both pier and sediment results
top10items_perHabitat <- top_items_per_habitat %>% 
  mutate_if(is.numeric, format, digits=2) %>% #set 2 decimal points
  kable("html", booktabs = TRUE, #nice format
      col.names = c("ID", "Habitat", "Sum", "SD", "Name"),
      caption = "Top 10 items per Habitat Type") %>% 
      pack_rows(index=c("Pier" = 10, "Sediment"=10)) %>%
      kable_styling(bootstrap_options="condensed", position="left") 

top10items_perHabitat #display table

#save table as png
save_kable(top10items_perHabitat, "figures/top10items_perHabitat.png") 

#Attempt to set tables side by side didn't work
#Reference https://bookdown.org/yihui/rmarkdown-cookbook/kable.html
# knitr::kables(
#   list(kable(top_items_per_pier, 
#              "html",
#              col.names = c("ID", "Habitat", "Sum", "SD", "Name"),
#              caption = 'Top 10 items at pier sites',
#              booktabs = TRUE, 
#              valign = 't',
#              digits=2),
#        kable(top_items_per_ss, 
#              "html",
#              col.names = c("ID", "Habitat", "Sum", "SD", "Name"),
#              caption = 'Top 10 items at pier sites',
#              booktabs = TRUE, 
#              valign = 't',
#              digits=2)) %>%
#     kable_styling(bootstrap_options = "condensed"))
```


```{r fishing-by-site}
#Isolate fishing vs non-fishing data
fishing <- site_select %>%
  group_by(Site, Fishing) %>% #group by fishing or non-fishing
  summarise(sum=sum(Total), #total fishing items
            mean=sum/unique(NumberSurveys),
            sd=sd(Total)) %>% #total per survey
  arrange(desc(mean))

site_habitat <- site_select %>%
  dplyr::select(Site, Habitat, Location) %>%
  distinct() #create df for labeling

fishing <- left_join(fishing, site_habitat, by="Site") #join for labeling

write.csv(fishing, "output/fishing.csv", row.names = FALSE) #save this data frame as a csv file

p_fishing <- fishing %>%
  ggplot( aes(x=Habitat, y=mean, fill=Fishing)) +
  geom_bar(position="dodge", stat="identity") +
  geom_errorbar(aes(ymin=mean-sd, ymax=mean+sd, width=0.2), position=position_dodge(width=0.90)) +
  facet_wrap(~Location, scales = "free_y")

p_fishing

ggsave(filename = "figures/fishing.png", plot = p_fishing, dpi = 600)
```


```{r habitat-by-site}
habitat <- site_select 
habitat$Material <- as.character(habitat$Material) #change Material to character
habitat$Material[habitat$Material %in% var_change] = "Other" #set categories to Other

habitat <- habitat %>%
  group_by(Location, Habitat) %>%
  summarise(sum=sum(Total), #total number of each material type per site
            mean=sum/unique(NumberSurveys), #mean of each material type per site
            sd=sd(Total)) %>% #sd of each material type per site
  arrange(desc(mean))

write.csv(habitat, "output/habitat.csv", row.names = FALSE) #save this data frame as a csv file

p_habitat <- habitat %>%
  ggplot( aes(x=Habitat, y=mean, fill=Habitat)) +
  geom_bar(position="dodge", stat="identity") +
  geom_errorbar(aes(ymin=mean-sd, ymax=mean+sd, width=0.2), position=position_dodge(width=0.90)) +
  theme(axis.text.x=element_text(color = "black", size=11, 
                                 vjust=.8, angle=30, hjust=0.8)) +
  scale_x_discrete(labels= HabitatLabs) +
  ylab("Mean item count by habitat type") + 
  ggtitle("Mean of debris items per site") +
  facet_wrap(~Location, labeller = labeller(Location=LocationLabs), scales="free_y")
p_habitat

ggsave(filename = "figures/habitat.png", plot = p_habitat, dpi = 600)
```


```{r debris-through-time}
#This accumulation graphic isn't really relevant without more surveys
site_select_sum <- site_select %>%
  group_by(Date, Location, Habitat) %>%
  summarise(sum=sum(Total), mean=mean(Total), sd=sd(Total))

    
timeline <- ggplot(site_select_sum, 
                   aes(x=Date, y=sum, group=Habitat, colour=Habitat)) +
      geom_line() +
      geom_point() +
      ggtitle("Debris count per Survey") +
      guides(colour=guide_legend("Item type")) +
      ylab("Total item count") +
      facet_wrap(~Location, labeller = labeller(Location=LocationLabs))
    timeline
    
    # ggsave(filename = "figures/timeline.png", plot = timeline, dpi = 600)
```
```{r boxplot}
#Find totals for each survey event
event_total_count <- site_select
event_total_count$Material <- as.character(event_total_count$Material) #change to character
event_total_count$Material[event_total_count$Material %in% var_change] = "Other" #set categories to Other

#Each sum value is a count per material type per survey date
event_total_count <- event_total_count %>%
group_by(Habitat, Location, Material, Date) %>%
mutate(sum=sum(Total), #total count per material group per survey date
          sd=sd(Total)) %>% #sd of material count per survey date
         ungroup()

 #simplify table
 event_total_count <- event_total_count %>%
   dplyr::select(Location, Habitat, Material, sum, sd) %>%
   distinct()

 p_boxplot <- event_total_count %>%
 ggplot( aes(Location, sum, fill=Habitat)) +
   geom_boxplot( aes(Location, sum),
                 outlier.shape=1, outlier.size=3) +
   geom_jitter(color="black", size=0.4, alpha=0.5) +
     theme(
       legend.position="none",
       plot.title = element_text(size=11)) +
     ggtitle("Debris counts by material per survey event") +
     ylab("Sum of each material type per survey event") +
     facet_wrap(~Location, labeller = labeller(Location=LocationLabs), scale="free")
 p_boxplot

 ggsave(filename = "figures/boxplot.png", plot = timeline, dpi = 600)
```
## Instead of using MDS for looking at the similarity of different ecological communities based on their species composition, I could look at the similarity of different debris item types based on their habitat. The abundance of all debris item types within replicated samples (plots or quadrats in the field, different sites etc.) is recorded and the raw data takes the form of a matrix of species (the variables) by samples.
```{r multi-dimensional-scaling}
#For reference:
#http://environmentalcomputing.net/multidimensional-scaling/ OR
#https://www.statmethods.net/advstats/mds.html
```
## **Regression** analysis is meant to isolate the relationship between each independent variable (Habitat) and the dependent variable (Total debris).  The interpretation of a regression coefficient is that it represents the mean change in the dependent (response) variable for each 1 unit change in an independent (predictor) variable when you hold all of the other independent (predictor) variables constant. -- Zuur Notes
## **Normality** at each predictor variable (X) should be checked by making a histogram of all observations at that particular X value. Count data is not normal (continuous and bell-shaped), so we use poisson regression and GLM.
```{r testing-normality}
hist <- site_select %>%
  group_by(Date, Location, Habitat) %>%
  summarise(sum=sum(Total))
hist

#To calculate number of bins, take SQRT of the total number of data entries SQRT 36=6
#To calculate binwidth, take the upper-lower entry / BINS.  602-0 / 6 = 100.

ggplot(hist, aes(sum)) +
  geom_histogram(bins=6, binwidth=100, alpha=0.5, position="identity")

ggplot(hist, aes(sum)) +
  geom_histogram(bins=6, binwidth=100, alpha=0.5, position="identity") +
  facet_grid(~Habitat)
```
## **GLM** deals with discrete rather than continuous data, and poisson deals with count data (not normal). The estimate is in log scale so must be exponentiated to be positive.  Z value is the estimate / standard error and it explains how strong the effect is.  Z value taken in consideration with degrees of freedom (# of data points) gives the P value.
```{r glm-1}
#add distance to harbour mouth to site_select data
site_attr <- site_attr %>%
  dplyr::select(Site, CostDis) #select only distance to harbour mouth for join
site_select <- left_join(site_select, site_attr, by="Site") #join distance to mouth to main dataset

# #GLM poisson with one variable
# m1 <- glm(Total ~ Habitat, #Total predicted by Habitat
#           data = site_select, 
#           family = "poisson") #poisson because it's count data
# summary(m1)
# tidy(m1)
# 
# m1_coef <- coef(m1)
# confint(m1)
# exp(m1_coef) #odds ratio is <1 (0.08), so there is less chance of finding debris on soft sediment sites
# plot(m1)

#GLM negative binomial with one variable
m1 <- glmer.nb(Total ~ Habitat + (1 | Location),#total by habitat, location random factor
               family="poisson", #poisson b/c count data
               data=site_select)

            
summary(m1)
plot(m1)

m1_coef <- coef(m1)
confint(m1)
exp(m1_coef) #odds ratio is <1 (0.08), so there is less chance of finding debris on soft sediment sites
plot(m1)


```

##Another way to look at residuals - this is what you'll use for GLMM so better to use this method:

```{r simulation-output-m1}
simulationOutput1 <- simulateResiduals(fittedModel = m1, plot=T)

testUniformity(simulationOutput1) #tests if the overall distribution conforms to expectations
testOutliers(simulationOutput1) #tests if there are more simulation outliers than expected
testDispersion(simulationOutput1) #tests if the simulated dispersion is equal to the observed dispersion
testQuantiles(simulationOutput1) #fits a quantile regression or residuals against a predictor (default predicted value), and tests of this conforms to the expected quantile
testZeroInflation(simulationOutput1) #tests if there are more zeros than expected

# #need to set a function for the summary in order to test particular scenarios - why does it look the same as dispesion test?
# means <- function(x) mean(x) # testing if mean prediction fits
# testGeneric(simulationOutput1, summary = countOnes) #test if a generic summary statistics deviates from model expectations

testTemporalAutocorrelation(simulationOutput1) #tests for temporal autocorrelation in the residuals
testSpatialAutocorrelation(simulationOutput1) #tests for spatial autocorrelation in the residuals. Can also be used with a generic distance function
```
## Use a sample to come up with intercept and slope estimates and confidence intervals. The confidence intervals tell us that if we repeat the experiment many times, how often are the intercept and slope are in the interval based on the confidence bands. A typical choice is 95% confidence interval.  The slope is the main thing that can tell us whether there's a relationship between the response and predictor variable. 
## **QQ Plot** compares a sample with a theoretical sample that comes from a certain distribution ex. the normal distribution.
## **Residuals** are the difference between the actual observed response values and the response values that the model predicted. -- Zuur notes
```{r glm-2}

#create df to use for models
mod <- site_select %>%
  mutate(DistKm = CostDis/100) %>%
  group_by(Location, Habitat, DistKm, NumberSurveys, ID, Fishing) %>% #relevant variables
  summarise(Total=sum(Total)) 

#GLM negative binomial with two variables and zero inflation
m2  <- glmmTMB(Total ~ #Debris count
                 Fishing*Habitat + #predicted by Habitat (fishing or non-fishing)
                 Fishing*DistKm + #predicted by distance from mouth (fishing or non-fishing)
                 (1|Location) + #site random effect
                 (1|ID) + #debris type random effect
                 offset(log(NumberSurveys)), #offset by survey count
               zi = ~1, #zero inflation
               family=nbinom1(), #negative binomial to deal with count data with lots of zeros
               data = mod) 

summary(m2)

#Range of dots on the residuals vs fitted is overdispersion, it's not accounting for the mean variance relationship correctly.  One of our assumptions is that variance is constant.  To fix this, use negative binomial instead of poisson. NBin takes into consideration the dispersion parameter.
```
```{r simulation-output-m2}
simulationOutput2 <- simulateResiduals(fittedModel = m2, plot=T)

plotResiduals(simulationOutput2)

testUniformity(simulationOutput2) #tests if the overall distribution conforms to expectations
testOutliers(simulationOutput2) #tests if there are more simulation outliers than expected
testDispersion(simulationOutput2) #tests if the simulated dispersion is equal to the observed dispersion
testQuantiles(simulationOutput2) #fits a quantile regression or residuals against a predictor (default predicted value), and tests of this conforms to the expected quantile
testZeroInflation(simulationOutput2) #tests if there are more zeros than expected

# #need to set a function for the summary in order to test particular scenarios - still not sure about this one...
# means <- function(x) mean(x) # testing if mean prediction fits
# testGeneric(simulationOutput2, summary = means) #test if a generic summary statistics deviates from model expectations

testTemporalAutocorrelation(simulationOutput2) #tests for temporal autocorrelation in the residuals
testSpatialAutocorrelation(simulationOutput2) #tests for spatial autocorrelation in the residuals. Can also be used with a generic distance function
```
## **Cook's Distance** is used to identify influential data points or outliers - a way to identify points that negatively effect the regression model. The measurement is a combination of each observation’s leverage and residual values; the higher the leverage and residuals, the higher the Cook’s distance. In this case there are no observations with a Cook distance larger than 1, which is the threshold that one should take further action. 
```{r cooks-distance}
#Residuals vs Fitted checks for homogeneity of the variance and the linear relation. (There should be no pattern for assumptions to be met, but there is a pattern.)
#QQ checks for the normal distribution of the residuals, and points should fall on a line. (It's not a line).
#Scale Location is the same as Residuals vs Fitted but with the residuals sqrt standardised.
#Cooks Distance detects points that have an overly large impact on the regression coefficients.
#Reference: https://biologyforfun.wordpress.com/2014/04/16/checking-glm-model-assumptions-in-r/

#plot Cook's Distance - why is this different than plot(m1)? 
CD_m1 <- cooks.distance(m1)
plot(CD_m1)

CD_m2 <- cooks.distance(m2)
plot(CD_m2)
```

## **Multicollinearity** is a problem when the order of correlated predictor values change the estimates for the regression coefficients. In this data there is a danger that distance to sea is correlated to whether the debris is fishing related (as more fishing tends to occur nearer the harbour mouth) and the habitat (as lightweight soft sediment debris is likely to be carried out to sea the closer to the harbour mouth)
```{r testing-multicollinearity}
# as.numeric(site_select$Fishing)
# cor.test(site_select$Fishing, site_select$CostDis)
# #Estimate 0.003335625, p-value 0.8505
# 
# as.numeric(site_select$Habitat)
# cor.test(site_select$Habitat, site_select$CostDis)
# #Estimate -0.005803366, p-value 0.742

```


```{r multicollinearity-plot}
# ggplot(data = site_select_sum, aes(x = sum, y = Habitat)) + 
# 	geom_jitter(width = 0, height = 0.05) +
# 	ylab("Probability of finding debris") +
# 	xlab("Pier vs Soft Sediment") +
#   geom_smooth()
```
## Multivariate analysis - each survey as a collection of debris is included as one point.  Treat debris items as a community - reduces everything into one datapoint at a survey.
```{r multivariate-analysis}
#For reference:
#http://environmentalcomputing.net/introduction-to-mvabund/
#https://cran.r-project.org/web/packages/vegan/vignettes/intro-vegan.pdf

#Find totals for each survey event
event_total <- site_select
event_total$Material <- as.character(event_total$Material) #change to character
event_total$Material[event_total_count$Material %in% var_change] = "Other" #set categories to Other

#find totals per material type for each habitat and survey event
event_total <- event_total %>%
group_by(Location, Habitat, Material, Date) %>%
mutate(sum=sum(Total)) #total count per material group per survey date

#convert to wide table
 event_total <- event_total %>%
   dplyr::select(Date, Location, Habitat, Material, sum) %>%
   distinct()
 
 data_wide <- spread(event_total_count, Material, sum)

#create mvabund object
mat_wide <- mvabund(data_wide[,4:7])

#look at the spread of the data
par(mar=c(2,10,2,2)) # adjusts the margins
boxplot(data_wide[,4:7],horizontal = TRUE,las=2, main="Abundance")



```


```{r glm-lightweight-debris-distance}
top_lgtwgt <- site_select %>%
  filter(ID %in% c("BP3", "S2", "S6", "H10", "S7"))
  top_lgtwgt

#negative binomial GLM with two variables
m_lgtwgt <- glm.nb(Total ~ Habitat + CostDis, #Total predicted by habitat and distance
          data = top_lgtwgt)
summary(m_lgtwgt)
tidy(m_lgtwgt)

plot(m_lgtwgt)
```
```{r glm-fishing-distance}
fishing_select <- site_select %>%
  filter(Fishing=="Fishing")

#negative binomial GLM with one variable
m_fishing <- glm.nb(Total ~ Habitat + CostDis, #Total predicted by Habitat
          data = fishing_select)
summary(m_fishing)
tidy(m_fishing)

plot(m_fishing)
```
```{r glm-sediment-distance}
ss_select <- site_select %>%
  filter(Habitat=="SS")

#GLM with one variable
m_ss <- glm.nb(Total ~ CostDis, #Total predicted by distance
          data = ss_select)
summary(m_ss)
tidy(m_ss)

plot(m_ss)
```
