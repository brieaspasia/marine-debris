---
title: "thesis-keywords"
author: "Brie Sherow"
date: "17/10/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r load-libraries, warning=FALSE, message=FALSE, results='hide'}
#for working with scopus metadata
library(bibliometrix)
#for cleaning and processing data
library(tidyverse)
library(data.table)
library(dplyr)
#for graphing
library(ggplot2)
#for clean display of tables and code
library(knitr)
library(kableExtra)
#netowrk manipulation
library(ggnet)
library(network)
library(sna)
# devtools::install_github("briatte/ggnet")
library(networkD3) #sankey diagram
```

```{r scopus-colnames}
read.csv('data/scopus_colnames.csv', header=FALSE, fileEncoding="UTF-8-BOM") %>%
  kable("html", col.names = c("Scopus Code", "Meaning")) %>% kable_styling() %>% #format table
  scroll_box(width = "100%", height = "500px") #format scroll box
```

## Data Exploration

### Loading SCOPUS dataset
```{r load-data, results='hide'}
# Load bibtex files and format into bibliometrix object
bib.71 <- convert2df(file = c("./data/70-99.bib", "./data/00.bib"), dbsource = "scopus", format = "bibtex")

bib.01 <- convert2df(file = c("./data/01-05.bib", "./data/06-10.bib"), dbsource = "scopus", format = "bibtex")

bib.11 <- convert2df(file = c("./data/11-14.bib", "./data/17_20.bib", "./data/15-16_18.bib", "./data/19.bib"), dbsource = "scopus", format = "bibtex")


save(bib.71, file = "./data/bib_71.RData") #save this data frame as RData
save(bib.01, file = "./data/bib_01.RData") #save this data frame as RData
save(bib.11, file = "./data/bib_11.RData") #save this data frame as RData
```

### Publications analysis
```{r analysis-and-summary}
results.71 <- biblioAnalysis(bib.71, sep=";") #high level analysis of scopus metadata
save(results.71, file = "./data/results_71.RData") #save this bibliometrix file as RData
options(width=100) #plot max number of columns
S.71 <- summary(object = results.71, k = 10, pause = FALSE) #extracting biblioAnalysis results into a list

results.01 <- biblioAnalysis(bib.01, sep=";") #high level analysis of scopus metadata
save(results.01, file = "./data/results_01.RData") #save this bibliometrix file as RData
options(width=100) #plot max number of columns
S.01 <- summary(object = results.01, k = 10, pause = FALSE) #extracting biblioAnalysis results into a list

results.11 <- biblioAnalysis(bib.11, sep=";") #high level analysis of scopus metadata
save(results.11, file = "./data/results_11.RData") #save this bibliometrix file as RData
options(width=100) #plot max number of columns
S.11 <- summary(object = results.11, k = 10, pause = FALSE) #extracting biblioAnalysis results into a list
```

### Annual increase in publications
```{r publications-per-year, results='hide'}
#total articles published per year
annual_prod <- print(S$AnnualProduction) #extracting df from summary list
names(annual_prod)[1] <- "Year" #rename to clean leading and trailing whitespace 
as.Date(annual_prod$Year, format="%Y") #set year to date format

#Create clean graph to annotate later
 ggplot(annual_prod, aes(Year, Articles, group=1)) + #visualise articles per year
          geom_point() + #set points to show each year
          geom_line() + #set line to show trends over time
     #declutter labels
          labs(title = "Marine Debris Research", subtitle = "articles published per year", x="Year", y="Articles") +
          scale_x_discrete(breaks = seq(1980, 2020, by = 5)) + 
          theme(axis.text.x = element_text(size = 8, angle = 90),
                axis.text.y = element_text(size = 8),
                panel.background = element_blank(),
                axis.title.y = element_text(size = rel(1.8)),
                axis.title.x = element_text(size = rel(1.8)),
                plot.title = element_text(lineheight=3, face="bold", color="black", size=18),
                plot.subtitle = element_text(lineheight=3, face="bold", color="black", size=12))

#save output into png graphic
ggsave(filename="figures/papers_per_year.png", width=11, height=9.5, units ="in")
```

```{r topics-total}
#load bibliometrix object
load(file = "./data/bib_71.RData") #load the bibliometrix object
load(file = "./data/bib_01.RData") #load the bibliometrix object
load(file = "./data/bib_11.RData") #load the bibliometrix object

bib.71 <- bib.71 %>%
  mutate(stage = "early") %>%
  dplyr::select(DE, ID, stage)

bib.01 <- bib.01 %>%
  mutate(stage = "mid") %>%
  dplyr::select(DE, ID, stage)

bib.11 <- bib.11 %>%
  mutate(stage = "recent") %>%
  dplyr::select(DE, ID, stage)

# bib <- rbind(bib.71, bib.01, bib.11)

#early

      head(bib.71$DE) #check data
      early_DE <- bib.71$DE #assign data
      early_DE <- unlist(strsplit(early_DE, ";")) #split the records into individual keywords
      early_DE <- early_DE[order(early_DE)] #order alphabetically
      early_DE <- gsub("^\\s+|\\s+$", "", early_DE) #trim white space
      table_early_DE <- table(early_DE) #determine frequency of words
      table_early_DE <- as.data.frame(table_early_DE) #transform to df
      table_early_DE$Keyword = table_early_DE$early_DE
      early_DE <- table_early_DE %>%
        select(Keyword, Freq)
      
      #rename early to keyword, and then merge ID and DE datasets, adding up totals for duplicate words
      
      head(bib.71$ID) #check data
      early_ID <- bib.71$ID #assign data
      early_ID <- unlist(strsplit(early_ID, ";")) #split the records into individual keywords
      early_ID <- early_ID[order(early_ID)] #order alphabetically
      early_ID <- gsub("^\\s+|\\s+$", "", early_ID) #trim white space
      table_early_ID <- table(early_ID) #determine frequency of words
      table_early_ID <- as.data.frame(table_early_ID) #transform to df
      table_early_ID$Keyword = table_early_ID$early_ID
      early_ID <- table_early_ID %>%
        select(Keyword, Freq)
      
      keyword_early <- 
        rbind(early_DE, early_ID) #append author words to journal words
      keyword_early <- keyword_early %>%
        mutate(stage="early")
      
#mid

      head(bib.01$DE) #check data
      mid_DE <- bib.01$DE #assign data
      mid_DE <- unlist(strsplit(mid_DE, ";")) #split the records into individual keywords
      mid_DE <- mid_DE[order(mid_DE)] #order alphabetically
      mid_DE <- gsub("^\\s+|\\s+$", "", mid_DE) #trim white space
      table_mid_DE <- table(mid_DE) #determine frequency of words
      table_mid_DE <- as.data.frame(table_mid_DE) #transform to df
      table_mid_DE$Keyword = table_mid_DE$mid_DE
      mid_DE <- table_mid_DE %>%
        select(Keyword, Freq)
      
      #rename mid to keyword, and then merge ID and DE datasets, adding up totals for duplicate words
      
      head(bib.01$ID) #check data
      mid_ID <- bib.01$ID #assign data
      mid_ID <- unlist(strsplit(mid_ID, ";")) #split the records into individual keywords
      mid_ID <- mid_ID[order(mid_ID)] #order alphabetically
      mid_ID <- gsub("^\\s+|\\s+$", "", mid_ID) #trim white space
      table_mid_ID <- table(mid_ID) #determine frequency of words
      table_mid_ID <- as.data.frame(table_mid_ID) #transform to df
      table_mid_ID$Keyword = table_mid_ID$mid_ID
      mid_ID <- table_mid_ID %>%
        select(Keyword, Freq)
      
      keyword_mid <- 
        rbind(mid_DE, mid_ID) #append author words to journal words
      keyword_mid <- keyword_mid %>%
        mutate(stage="mid")
      
#recent

      head(bib.11$DE) #check data
      recent_DE <- bib.11$DE #assign data
      recent_DE <- unlist(strsplit(recent_DE, ";")) #split the records into individual keywords
      recent_DE <- recent_DE[order(recent_DE)] #order alphabetically
      recent_DE <- gsub("^\\s+|\\s+$", "", recent_DE) #trim white space
      table_recent_DE <- table(recent_DE) #determine frequency of words
      table_recent_DE <- as.data.frame(table_recent_DE) #transform to df
      table_recent_DE$Keyword = table_recent_DE$recent_DE
      recent_DE <- table_recent_DE %>%
        select(Keyword, Freq)
      
      #rename recent to keyword, and then merge ID and DE datasets, adding up totals for duplicate words
      
      head(bib.11$ID) #check data
      recent_ID <- bib.11$ID #assign data
      recent_ID <- unlist(strsplit(recent_ID, ";")) #split the records into individual keywords
      recent_ID <- recent_ID[order(recent_ID)] #order alphabetically
      recent_ID <- gsub("^\\s+|\\s+$", "", recent_ID) #trim white space
      table_recent_ID <- table(recent_ID) #determine frequency of words
      table_recent_ID <- as.data.frame(table_recent_ID) #transform to df
      table_recent_ID$Keyword = table_recent_ID$recent_ID
      recent_ID <- table_recent_ID %>%
        select(Keyword, Freq)
      
      keyword_recent <- 
        rbind(recent_DE, recent_ID) #append author words to journal words
      keyword_recent <- keyword_recent %>%
        mutate(stage="recent")
      
keyword_total <- rbind(keyword_early, keyword_mid, keyword_recent)

keywords <- keyword_total %>%
  group_by(Keyword, stage) %>% #find duplicate matches
  mutate(Total=sum(Freq)) %>% #add duplicate occurrences
           ungroup() %>%
  select(-Freq) %>%
  distinct()

keyword_code <- read.csv(file="data/word_code.csv", 
                         header=T, sep=",")

keyword_code <- keyword_code %>%
  dplyr::select(Keyword, Code)

t <- left_join(keywords, keyword_code, by = "Keyword")

tt <- t %>%
  filter(Code != "") %>%
  filter(Code != "n" & Code != "u") %>%
  group_by(Code, stage) %>%
  summarise(sum = sum(Total))

ggplot(tt, aes(Code, sum, fill = stage)) +
  geom_col()
         

word_code <- t %>%
  dplyr::select(Keyword, Total, Code) %>%
  group_by(Keyword) %>% #find duplicate matches
  mutate(Total=sum(Total)) %>% #add duplicate occurrences
           ungroup() %>%
  distinct()

write.csv(word_code, "data/word_code.csv", row.names = FALSE) #save this data frame as a csv file

#rename the count to freq for the wordcloud
colnames(keywords)[colnames(keywords) == "Keyword"] <- "word" #rename for wordcloud2
colnames(keywords)[colnames(keywords) == "Total"] <- "freq" #rename for wordcloud2
keywords <- keywords %>% dplyr::filter(freq>2) #remove low values

 #writing wordcloud graphic
  wc_total <- wordcloud2(keywords, shape='rectangle')
  wc_total
```

```{r networks}
load(file = "./data/bib_71.RData") #load the bibliometrix object


NetMatrix <- biblioNetwork(bib.71, n=100, analysis = "co-occurrences", network = "author_keywords", sep = ";") #create keyword network
net <- networkPlot(NetMatrix, normalize="association", n = 30, Title = "Keyword Co-occurrences", type = "fruchterman", size.cex=TRUE, size=20, remove.multiple=F, edgesize = 10, edges.min=2) #plot keyword networks
net

net <- as.matrix(NetMatrix)

library(igraph)
g <- graph.adjacency(net)
edgelist <- get.edgelist(g)

sankeyNetwork(Links = x, Nodes = sources)


ggnet2(NetMatrix)

# https://www.jessesadler.com/post/network-analysis-with-r/
#creating node and edge lists


#https://briatte.github.io/ggnet/ #network objects in ggplot
#ggnet places nodes using "fruchterman-reingold" by default
```

```{r tutorial}
#http://www.kateto.net/wp-content/uploads/2015/06/Polnet%202015%20Network%20Viz%20Tutorial%20-%20Ognyanova.pdf
nodes <-read.csv("data/network/Dataset1-Media-Example-NODES.csv", header=T, as.is=T)
links <-read.csv("data/network/Dataset1-Media-Example-EDGES.csv", header=T, as.is=T)

links <-aggregate(links[,3], links[,-3], sum)
links <- links[order(links$from, links$to),]
colnames(links)[4] <- "weight"
rownames(links) <- NULL

nodes2 <-read.csv("data/network/Dataset2-Media-User-Example-NODES.csv", header=T, as.is=T)
links2 <-read.csv("data/network/Dataset2-Media-User-Example-EDGES.csv", header=T, row.names=1)

links2 <-as.matrix(links2)
dim(links2)
dim(nodes2)
```

