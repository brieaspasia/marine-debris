---
title: "thesis-keywords"
author: "Brie Sherow"
date: "17/10/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r load-libraries, warning=FALSE, message=FALSE, results='hide'}
#for working with scopus metadata
library(bibliometrix)
#for cleaning and processing data
library(tidyverse)
library(data.table)
library(dplyr)
library(tidyr)
#for graphing
library(ggplot2)
#for clean display of tables and code
library(knitr)
library(kableExtra)
#netowrk manipulation
library(ggnet)
library(network)
library(sna)
library(spdep)
# devtools::install_github("briatte/ggnet")
library(networkD3) #sankey diagram
library(igraph)
library(viridis)
```

```{r scopus-colnames}
read.csv('data/scopus_colnames.csv', header=FALSE, fileEncoding="UTF-8-BOM") %>%
  kable("html", col.names = c("Scopus Code", "Meaning")) %>% kable_styling() %>% #format table
  scroll_box(width = "100%", height = "500px") #format scroll box
```

## Data Exploration

### Loading SCOPUS dataset
```{r load-data, results='hide'}
# Load bibtex files and format into bibliometrix object
bib.71 <- convert2df(file = c("./data/70-99.bib", "./data/00.bib"), dbsource = "scopus", format = "bibtex")

bib.01 <- convert2df(file = c("./data/01-05.bib", "./data/06-10.bib"), dbsource = "scopus", format = "bibtex")

bib.11 <- convert2df(file = c("./data/11-14.bib", "./data/17_20.bib", "./data/15-16_18.bib", "./data/19.bib"), dbsource = "scopus", format = "bibtex")


save(bib.71, file = "./data/bib_71.RData") #save this data frame as RData
save(bib.01, file = "./data/bib_01.RData") #save this data frame as RData
save(bib.11, file = "./data/bib_11.RData") #save this data frame as RData
```

### Publications analysis
```{r analysis-and-summary}
results.71 <- biblioAnalysis(bib.71, sep=";") #high level analysis of scopus metadata
save(results.71, file = "./data/results_71.RData") #save this bibliometrix file as RData
options(width=100) #plot max number of columns
S.71 <- summary(object = results.71, k = 10, pause = FALSE) #extracting biblioAnalysis results into a list

results.01 <- biblioAnalysis(bib.01, sep=";") #high level analysis of scopus metadata
save(results.01, file = "./data/results_01.RData") #save this bibliometrix file as RData
options(width=100) #plot max number of columns
S.01 <- summary(object = results.01, k = 10, pause = FALSE) #extracting biblioAnalysis results into a list

results.11 <- biblioAnalysis(bib.11, sep=";") #high level analysis of scopus metadata
save(results.11, file = "./data/results_11.RData") #save this bibliometrix file as RData
options(width=100) #plot max number of columns
S.11 <- summary(object = results.11, k = 10, pause = FALSE) #extracting biblioAnalysis results into a list
```

### Annual increase in publications
```{r publications-per-year, results='hide'}
#total articles published per year
annual_prod <- print(S$AnnualProduction) #extracting df from summary list
names(annual_prod)[1] <- "Year" #rename to clean leading and trailing whitespace 
as.Date(annual_prod$Year, format="%Y") #set year to date format

#Create clean graph to annotate later
 ggplot(annual_prod, aes(Year, Articles, group=1)) + #visualise articles per year
          geom_point() + #set points to show each year
          geom_line() + #set line to show trends over time
     #declutter labels
          labs(title = "Marine Debris Research", subtitle = "articles published per year", x="Year", y="Articles") +
          scale_x_discrete(breaks = seq(1980, 2020, by = 5)) + 
          theme(axis.text.x = element_text(size = 8, angle = 90),
                axis.text.y = element_text(size = 8),
                panel.background = element_blank(),
                axis.title.y = element_text(size = rel(1.8)),
                axis.title.x = element_text(size = rel(1.8)),
                plot.title = element_text(lineheight=3, face="bold", color="black", size=18),
                plot.subtitle = element_text(lineheight=3, face="bold", color="black", size=12))

#save output into png graphic
ggsave(filename="figures/papers_per_year.png", width=11, height=9.5, units ="in")
```

```{r topics-total}
#load bibliometrix object
load(file = "./data/bib_71.RData") #load the bibliometrix object
load(file = "./data/bib_01.RData") #load the bibliometrix object
load(file = "./data/bib_11.RData") #load the bibliometrix object

bib.71 <- bib.71 %>%
  mutate(stage = "early") %>%
  dplyr::select(DE, ID, stage)

bib.01 <- bib.01 %>%
  mutate(stage = "mid") %>%
  dplyr::select(DE, ID, stage)

bib.11 <- bib.11 %>%
  mutate(stage = "recent") %>%
  dplyr::select(DE, ID, stage)

# bib <- rbind(bib.71, bib.01, bib.11)

#early

      head(bib.71$DE) #check data
      early_DE <- bib.71$DE #assign data
      early_DE <- unlist(strsplit(early_DE, ";")) #split the records into individual keywords
      early_DE <- early_DE[order(early_DE)] #order alphabetically
      early_DE <- gsub("^\\s+|\\s+$", "", early_DE) #trim white space
      table_early_DE <- table(early_DE) #determine frequency of words
      table_early_DE <- as.data.frame(table_early_DE) #transform to df
      table_early_DE$Keyword = table_early_DE$early_DE
      early_DE <- table_early_DE %>%
        select(Keyword, Freq)
      
      #rename early to keyword, and then merge ID and DE datasets, adding up totals for duplicate words
      
      head(bib.71$ID) #check data
      early_ID <- bib.71$ID #assign data
      early_ID <- unlist(strsplit(early_ID, ";")) #split the records into individual keywords
      early_ID <- early_ID[order(early_ID)] #order alphabetically
      early_ID <- gsub("^\\s+|\\s+$", "", early_ID) #trim white space
      table_early_ID <- table(early_ID) #determine frequency of words
      table_early_ID <- as.data.frame(table_early_ID) #transform to df
      table_early_ID$Keyword = table_early_ID$early_ID
      early_ID <- table_early_ID %>%
        select(Keyword, Freq)
      
      keyword_early <- 
        rbind(early_DE, early_ID) #append author words to journal words
      keyword_early <- keyword_early %>%
        mutate(stage="early")
      
#mid

      head(bib.01$DE) #check data
      mid_DE <- bib.01$DE #assign data
      mid_DE <- unlist(strsplit(mid_DE, ";")) #split the records into individual keywords
      mid_DE <- mid_DE[order(mid_DE)] #order alphabetically
      mid_DE <- gsub("^\\s+|\\s+$", "", mid_DE) #trim white space
      table_mid_DE <- table(mid_DE) #determine frequency of words
      table_mid_DE <- as.data.frame(table_mid_DE) #transform to df
      table_mid_DE$Keyword = table_mid_DE$mid_DE
      mid_DE <- table_mid_DE %>%
        select(Keyword, Freq)
      
      #rename mid to keyword, and then merge ID and DE datasets, adding up totals for duplicate words
      
      head(bib.01$ID) #check data
      mid_ID <- bib.01$ID #assign data
      mid_ID <- unlist(strsplit(mid_ID, ";")) #split the records into individual keywords
      mid_ID <- mid_ID[order(mid_ID)] #order alphabetically
      mid_ID <- gsub("^\\s+|\\s+$", "", mid_ID) #trim white space
      table_mid_ID <- table(mid_ID) #determine frequency of words
      table_mid_ID <- as.data.frame(table_mid_ID) #transform to df
      table_mid_ID$Keyword = table_mid_ID$mid_ID
      mid_ID <- table_mid_ID %>%
        select(Keyword, Freq)
      
      keyword_mid <- 
        rbind(mid_DE, mid_ID) #append author words to journal words
      keyword_mid <- keyword_mid %>%
        mutate(stage="mid")
      
#recent

      head(bib.11$DE) #check data
      recent_DE <- bib.11$DE #assign data
      recent_DE <- unlist(strsplit(recent_DE, ";")) #split the records into individual keywords
      recent_DE <- recent_DE[order(recent_DE)] #order alphabetically
      recent_DE <- gsub("^\\s+|\\s+$", "", recent_DE) #trim white space
      table_recent_DE <- table(recent_DE) #determine frequency of words
      table_recent_DE <- as.data.frame(table_recent_DE) #transform to df
      table_recent_DE$Keyword = table_recent_DE$recent_DE
      recent_DE <- table_recent_DE %>%
        select(Keyword, Freq)
      
      #rename recent to keyword, and then merge ID and DE datasets, adding up totals for duplicate words
      
      head(bib.11$ID) #check data
      recent_ID <- bib.11$ID #assign data
      recent_ID <- unlist(strsplit(recent_ID, ";")) #split the records into individual keywords
      recent_ID <- recent_ID[order(recent_ID)] #order alphabetically
      recent_ID <- gsub("^\\s+|\\s+$", "", recent_ID) #trim white space
      table_recent_ID <- table(recent_ID) #determine frequency of words
      table_recent_ID <- as.data.frame(table_recent_ID) #transform to df
      table_recent_ID$Keyword = table_recent_ID$recent_ID
      recent_ID <- table_recent_ID %>%
        select(Keyword, Freq)
      
      keyword_recent <- 
        rbind(recent_DE, recent_ID) #append author words to journal words
      keyword_recent <- keyword_recent %>%
        mutate(stage="recent")
      
keyword_total <- rbind(keyword_early, keyword_mid, keyword_recent)

keywords <- keyword_total %>%
  group_by(Keyword, stage) %>% #find duplicate matches
  mutate(Total=sum(Freq)) %>% #add duplicate occurrences
           ungroup() %>%
  select(-Freq) %>%
  distinct()

keyword_code <- read.csv(file="data/word_code.csv", 
                         header=T, sep=",")

keyword_code <- keyword_code %>%
  dplyr::select(Keyword, Code)

t <- left_join(keywords, keyword_code, by = "Keyword")

tt <- t %>%
  filter(Code != "") %>%
  filter(Code != "n" & Code != "u") %>%
  group_by(Code, stage) %>%
  summarise(sum = sum(Total))

ggplot(tt, aes(Code, sum, fill = stage)) +
  geom_col()
         

word_code <- t %>%
  dplyr::select(Keyword, Total, Code) %>%
  group_by(Keyword) %>% #find duplicate matches
  mutate(Total=sum(Total)) %>% #add duplicate occurrences
           ungroup() %>%
  distinct()

write.csv(word_code, "data/word_code.csv", row.names = FALSE) #save this data frame as a csv file

#rename the count to freq for the wordcloud
colnames(keywords)[colnames(keywords) == "Keyword"] <- "word" #rename for wordcloud2
colnames(keywords)[colnames(keywords) == "Total"] <- "freq" #rename for wordcloud2
keywords <- keywords %>% dplyr::filter(freq>2) #remove low values

 #writing wordcloud graphic
  wc_total <- wordcloud2(keywords, shape='rectangle')
  wc_total
```

```{r networks}
load(file = "./data/bib_71.RData") #load the bibliometrix object


NetMatrix <- biblioNetwork(bib.71, n=100, analysis = "co-occurrences", network = "author_keywords", sep = ";") #create keyword network
# net <- networkPlot(NetMatrix, normalize="association", n = 30, Title = "Keyword Co-occurrences", type = "fruchterman", size.cex=TRUE, size=20, remove.multiple=F, edgesize = 10, edges.min=2) #plot keyword networks
# net

#from sparse matrix to adjacency matrix
net <- as.matrix(NetMatrix)

nodes <- as.data.frame(net) %>%
  rownames_to_column("label") %>%
  mutate(id = row_number()) %>%
  dplyr::select(id, label)


per_route <-as.data.frame(net) %>%
    rownames_to_column("From") %>%
    gather(key="To", value="Weight", 2:21) %>% 
    arrange(From, To)


#remove self comparisons
per_route <- subset(per_route, From != To)

edges <- per_route %>% 
  left_join(nodes, by = c("From" = "label")) %>% 
  rename(from = id)

edges <- edges %>% 
  left_join(nodes, by = c("To" = "label")) %>% 
  rename(to = id) %>%
  select(from, to, Weight)


routes_igraph <- graph_from_data_frame(d = edges, vertices = nodes, directed = TRUE)
plot(routes_igraph, edge.arrow.size = 0.2)

ggraph(routes_igraph, layout = "linear") + 
  geom_edge_arc(aes(width = Weight), alpha = 0.8) + 
  scale_edge_width(range = c(0.2, 2)) +
  geom_node_text(aes(label = label)) +
  labs(edge_width = "Weight") +
  theme_graph()

# better arc diagram https://www.data-to-viz.com/graph/arc.html order of nodes is important

#Sankey
nodes_d3 <- mutate(nodes, id = id - 1)
edges_d3 <- mutate(edges, from = from - 1, to = to - 1)

forceNetwork(Links = edges_d3, Nodes = nodes_d3, Source = "from", Target = "to", 
             NodeID = "label", Group = "id", Value = "Weight", 
             opacity = 1, fontSize = 16, zoom = TRUE)

sankeyNetwork(Links = edges_d3, Nodes = nodes_d3, Source = "from", Target = "to", 
              NodeID = "label", Value = "Weight", fontSize = 16, unit = "")



# https://www.jessesadler.com/post/network-analysis-with-r/
#creating node and edge lists


#https://briatte.github.io/ggnet/ #network objects in ggplot
#ggnet places nodes using "fruchterman-reingold" by default

# https://kateto.net/network-visualization
```
```{r arc-early}
#https://www.data-to-viz.com/graph/arc.html

load(file = "./data/bib_71.RData") #load the bibliometrix object

      #create sparse matrix
      NetMatrix <- biblioNetwork(bib.71, n=50, analysis = "co-occurrences", network = "author_keywords", sep = ";")
      
      #from sparse matrix to adjacency matrix
      net <- as.matrix(NetMatrix)
      
      #adjacency matrix to long form
      connect <- as.data.frame(net) %>%
        rownames_to_column("from") %>%
        gather(key = "to", value = "value", -1) %>%
        na.omit()
      
      connect <- subset(connect, from != to)
      connect <- subset(connect, value != 0)
      
      # Number of connection per person
      c( as.character(connect$from), as.character(connect$to)) %>%
        as.tibble() %>%
        group_by(value) %>%
        summarize(n=n()) -> keywords
      colnames(keywords) <- c("word", "n")
      
      write.csv(keywords, "data/keywords.71.csv", row.names=T)
    
      # Create a graph object with igraph
      mygraph <- graph_from_data_frame( connect, vertices = keywords, directed = FALSE )
      
      # Find community
      com <- walktrap.community(mygraph)
      #max(com$membership)
      
      keywords <- keywords %>% 
  mutate( grp = com$membership) %>%
  arrange(grp) %>%
  mutate(word=factor(word, word))
      
    
      
  #     #keep only 10 first communities (not relevant here)
  #     keywords <- keywords %>% 
  # filter(grp<16)
      
      mygraph <- graph_from_data_frame( connect, vertices = keywords, directed = FALSE )
      
      mygraph
      
      ??colormap
      
mycolor <- colormap(colormap=colormaps$viridis, nshades=max(keywords$grp))
mycolor <- sample(mycolor, length(mycolor))
      
ggraph(mygraph, layout="linear") + 
  geom_edge_arc(edge_colour="black", edge_alpha=0.2, edge_width=0.3, fold=TRUE) +
  geom_node_point(aes(size=n, color=as.factor(grp), fill=grp), alpha=0.5) +
  scale_size_continuous(range=c(0.5,8)) +
  scale_color_manual(values=mycolor) +
  geom_node_text(aes(label=name), angle=65, hjust=1, nudge_y = -1.1, size=2.3) +
  theme_void() +
  theme(
    legend.position="none",
    plot.margin=unit(c(0,0,0.4,0), "null"),
    panel.spacing=unit(c(0,0,3.4,0), "null")
  ) +
  expand_limits(x = c(-1.2, 1.2), y = c(-5.6, 1.2))

# need to determine a better order for nodes
# weighted arcs https://www.r-bloggers.com/2013/02/arc-diagrams-in-r-les-miserables/
#remove dud words like 'marine pollution'
```
```{r arc-recent}
#https://www.data-to-viz.com/graph/arc.html

load(file = "./data/bib_11.RData") #load the bibliometrix object

      #create sparse matrix
      NetMatrix <- biblioNetwork(bib.11, n=15, analysis = "co-occurrences", network = "author_keywords", sep = ";")
      
      #from sparse matrix to adjacency matrix
      net <- as.matrix(NetMatrix)
      
      #adjacency matrix to long form
      connect <- as.data.frame(net) %>%
        rownames_to_column("from") %>%
        gather(key = "to", value = "value", -1) %>%
        na.omit()
      
      connect <- subset(connect, from != to)
      connect <- subset(connect, value != 0)
      
      # Number of connection per person
      c( as.character(connect$from), as.character(connect$to)) %>%
        as.tibble() %>%
        group_by(value) %>%
        summarize(n=n()) -> keywords
      colnames(keywords) <- c("word", "n")
      
      write.csv(keywords, "data/keywords.11.csv", row.names=T)
      
      # Create a graph object with igraph
      mygraph <- graph_from_data_frame( connect, vertices = keywords, directed = FALSE )
      
      # Find community
      com <- walktrap.community(mygraph)
      #max(com$membership)
      
      keywords <- keywords %>% 
  mutate( grp = com$membership) %>%
  arrange(grp) %>%
  mutate(word=factor(word, word))
      
  #     #keep only 10 first communities (not relevant here)
  #     keywords <- keywords %>% 
  # filter(grp<16)
      
      mygraph <- graph_from_data_frame( connect, vertices = keywords, directed = FALSE )
      
      mygraph
      
      # prepare a vector of n color in the viridis scale
mycolor <- colormap(colormap=colormaps$viridis, nshades=max(keywords$grp))
mycolor <- sample(mycolor, length(mycolor))
      
ggraph(mygraph, layout="linear") + 
  geom_edge_arc(edge_colour="black", edge_alpha=0.2, edge_width=0.3, fold=TRUE) +
  geom_node_point(aes(size=n, color=as.factor(grp), fill=grp), alpha=0.5) +
  scale_size_continuous(range=c(0.5,8)) +
  scale_color_manual(values=mycolor) +
  geom_node_text(aes(label=name), angle=65, hjust=1, nudge_y = -1.1, size=2.3) +
  theme_void() +
  theme(
    legend.position="none",
    plot.margin=unit(c(0,0,0.4,0), "null"),
    panel.spacing=unit(c(0,0,3.4,0), "null")
  ) +
  expand_limits(x = c(-1.2, 1.2), y = c(-5.6, 1.2))

# need to determine a better order for nodes
# weighted arcs https://www.r-bloggers.com/2013/02/arc-diagrams-in-r-les-miserables/
#remove dud words like 'marine pollution'
```


```{r}

# https://rdrr.io/cran/spdep/f/vignettes/nb_igraph.Rmd
#spatial weights and sparse matrices tutorial

# https://rdrr.io/github/briatte/ggnet/f/vignettes/ggnet2.Rmd

net = network(connect, directed = TRUE)


ggnet2(net, mode = "kamadakawai")

# party affiliation
x = data.frame(word = network.vertex.names(net))
x = merge(x, keywords, by="word", sort = FALSE)$grp

net %v% "group" = as.character(x)

y = RColorBrewer::brewer.pal(9, "Set1")[ c(3, 1, 9, 6, 8, 5, 2) ]
names(y) = levels(x)

ggnet2(net, color = "group", palette = y, alpha = 0.75, size = 4, edge.alpha = 0.5)

# party affiliation
x = data.frame(Twitter = network.vertex.names(net))
x = merge(x, v, by = "Twitter", sort = FALSE)$Groupe
net %v% "party" = as.character(x)


# root URL
r = "https://raw.githubusercontent.com/briatte/ggnet/master/"

# read nodes
v = read.csv(paste0(r, "inst/extdata/nodes.tsv"), sep = "\t")
names(v)

# read edges
e = read.csv(paste0(r, "inst/extdata/network.tsv"), sep = "\t")
names(e)

#e = from | to
#v = keywords

```




