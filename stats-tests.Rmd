---
title: "stats-tests"
author: "Brie Sherow"
date: "11/11/2020"
output:
  html_document:
    toc: yes
    toc_float: yes
    code_folding: hide
    df_print: paged
  pdf_document:
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r load-libraries, warning=FALSE, message=FALSE, results='hide'}
library(ggplot2) #graphing
library(ggthemes) #graphing templates
library(hrbrthemes) #graphing templates
library(lubridate) #date manipulation
library(forcats) #working with factors
library(tidyverse) #manipulating data
library(knitr) #rmarkdown functions
library(kableExtra) #table layouts
library(magick) #image processing
library(stats) #R stats functions
library(broom) #create summaries from stats objects
library(car) #lm regression
library(MASS) #stats
library(lme4) #glmer function
library(DHARMa) #testing model diagnostics
library(glmmTMB) #fit zero-inflated negative binomial
library(mvabund) #for multivariate stats
library(gllvm) #for multivariate abundance
library(lattice) #fourth corner heatmap

#Tutorial
#https://cran.r-project.org/web/packages/gllvm/vignettes/vignette1.html
```
# Supplementary Materials 
The repository used to store this data can be found at [https://github.com/brieaspasia/marine-debris](https://github.com/brieaspasia/marine-debris)

# Site selection
10 sites at 5 locations were surveyed between 3-5 times each during summer 2019-2020.
```{r create-site-select}
#read in processed survey data
data <- read.csv(file="data/all_data.csv", 
                 header=T, sep=",", 
                 fileEncoding="UTF-8-BOM") #removes special characters

data$Date <- dmy(data$Date) 
data$Notes <- as.character(data$Notes)

#keep only unsw surveys and sites with public piers and >2 survey rounds
site_select <- data %>%
  filter(Team=="UNSW" & 
           Location %in% c("DOUBL","WATSO","PARSL","NEUTR","CLIFT")) %>%
  dplyr::select(-Habitat2, -Fragment, -Whole)

#create a df for survey count per site
sampling_dates <- data %>%
  group_by(Location) %>%
  filter(Team=="UNSW") %>%
  count(Date)

#take away n
sampling_dates$n <- NULL

#create a column for survey count per site
sampling_dates <- sampling_dates %>%
  count(Location) %>%
  arrange(desc(n)) %>%
  rename(NumberSurveys=n)

#join survey count to site select
site_select <- left_join(site_select, sampling_dates, by="Location")
```

# Debris Types
CSIRO debris classification system was modified to assign a material type and whether fishing or non-fishing related for each debris item.  Dispersiveness was added as a trait using Tangaroa Blue classification system.
```{r load-item-code}
#load item names
item_code <- read.csv(file="data/CSIRO-code.csv", header=T, sep=",") 

#filter out material as some item types may have more than one material type 
#(ex. fishing items, furniture)
item_code <- item_code %>%
  dplyr::select(ID, item_name, Fishing, Dispersiveness) 
```

# Site Attributes
Each location included a 25m transect at a pier and an adjacent 25m soft sediment site in an area of boat traffic.  Distance from harbour mouth was calculated using ArcGIS Cost Distance analysis.
```{r load-site-attributes}
#load site coords and distance from harbour mouth
site_attr <- read.csv(file="data/site_attributes.csv", header=T, sep=",", fileEncoding="UTF-8-BOM")

#add distance to harbour mouth to site_select data
site_attr <- site_attr %>%
  dplyr::select(Site, CostDis) %>% #select only distance to harbour mouth for join
  mutate(DistKm = CostDis/100) #distance to harbour mouth
  
#join distance to mouth to main dataset
site_select <- left_join(site_select, site_attr, by="Site") 
  
```

# GLMM model
Regression isolates the relationship between each independent variable (Habitat and distance) and the dependent variable (Total debris). Total debris count predicted by habitat (pier or sediment) and distance from harbour mouth, both affected by fishing and with location as a random effect.  Offset by the number of surveys at each site to account for differing counts.  Using negative binomial because it is count data with a high number of zeros, and an additional zero inflation because of the zeros.

Debris type is not considered as a random effect because in this instance it is the total debris count that matters rather than the individual debris types.  Accumulation due to repeat surveys is also not considered as the temporal autocorrelation does not indicate that repeat surveys are significant (see below).


```{r glm-2, warning=FALSE, message=FALSE}
#create df to use for models
mod <- site_select %>%
  group_by(Date, Location, Habitat, DistKm, NumberSurveys, 
           ID,
           Fishing) %>% #relevant variables
  summarise(Total=sum(Total)) 

#GLM negative binomial with two variables and zero inflation
m2  <- glmmTMB(Total ~ #Debris count
                 Fishing*Habitat + #predicted by Habitat (fishing or non-fishing)
                 Fishing*DistKm + #predicted by distance from mouth (fishing or non-fishing)
                 (1|Location) + #site random effect
                 (1|ID) + #debris type random effect
                 # ar1(Date + 0 | Location) +
                 offset(log(NumberSurveys)), #offset by survey count
               zi = ~1, #zero inflation
               family=nbinom1(), #negative binomial to deal with count data with lots of zeros
               data = mod) 
```
#Model summary
P values are all less than 0.05, indicating significance.

```{r model-summary}
summary(m2)
```

# Regression coefficients
This is the mean change in the dependent (total debris) variable for each 1 unit change in an independent (habitat and distance) variable.  For instance, each km more from the harbour results in -0.365 less debris items.  Soft sediment sites are likely to have -3.141 less count per item than pier sites.

#I'm not sure how to explain the coefficients on the fishing effect?  And is the intercept the pier sites?  I still get so confused about the intercept and how the model chooses it alphabetically...

```{r coefficients}
coef(m2)
```
# Testing temporal autocorrelation
Despite repeat surveys at each site, temporal autocorrelation testing shows that the effect of repeat surveys is not significant.  This is likely due to the fact that the initial surveys did not actually function as baseline cleans because the sites took many trips to clean and pier sites were never cleared completely.
```{r testing-temporal-autocorrelation, warning=FALSE, message=FALSE, results="hide"}
#creating a column from the model residuals
mod$resid = resid(m2) #create a column for each survey event's residuals

#Clifton Gardens - DW = 3.301, p-value = 0.05987
  m_clift <- mod %>%
    filter(Location=="CLIFT") %>% 
    group_by(Date) %>%
    summarise(sum=sum(resid))
  
  testTemporalAutocorrelation(m_clift$sum, 
                              time =  m_clift$Date) #test resid for temporal autocorrelation

#Double Bay - DW = 2.2347, p-value = 0.8496
  m_doubl <- mod %>%
    filter(Location=="DOUBL") %>%
    group_by(Date) %>%
    summarise(sum=sum(resid))
  
  testTemporalAutocorrelation(m_doubl$sum, 
                              time =  m_doubl$Date) #test resid for temporal autocorrelation

#Neutral Bay - DW = 2.767, p-value = 0.3581
  m_neutr <- mod %>%
    filter(Location=="NEUTR") %>%
    group_by(Date) %>%
    summarise(sum=sum(resid))
  
  testTemporalAutocorrelation(m_neutr$sum, 
                              time =  m_neutr$Date) #test resid for temporal autocorrelation
#Parsley Bay - DW = 1.2492, p-value = 0.4578
    m_parsl <- mod %>%
    filter(Location=="PARSL") %>%
    group_by(Date) %>%
    summarise(sum=sum(resid))
  
  testTemporalAutocorrelation(m_parsl$sum, 
                              time =  m_parsl$Date) #test resid for temporal autocorrelation
#Watsons Bay - DW = 2.8324, p-value = 0.3741
    m_watso <- mod %>%
    filter(Location=="WATSO") %>%
    group_by(Date) %>%
    summarise(sum=sum(resid))
  
  testTemporalAutocorrelation(m_watso$sum, 
                              time =  m_watso$Date) #test resid for temporal autocorrelation
```

# DHARMa Simulation Output
If I take out the debris type (ID) as a random effect, the residual graph lines are red.  If I leave in the debris type as a random effect then the KS test on the QQ plot is red.  I don't know why these are throwing errors when we had them working before?

```{r simulation-output-m2}
simulationOutput2 <- simulateResiduals(fittedModel = m2, plot=T)
  
plotResiduals(simulationOutput2)

# testUniformity(simulationOutput2) #tests if the overall distribution conforms to expectations
# testOutliers(simulationOutput2) #tests if there are more simulation outliers than expected
# testDispersion(simulationOutput2) #tests if the simulated dispersion is equal to the observed dispersion
# testQuantiles(simulationOutput2) #fits a quantile regression or residuals against a predictor (default predicted value), and tests of this conforms to the expected quantile
# testZeroInflation(simulationOutput2) #tests if there are more zeros than expected

# #need to set a function for the summary in order to test particular scenarios - still not sure about this one...
# means <- function(x) mean(x) # testing if mean prediction fits
# testGeneric(simulationOutput2, summary = means) #test if a generic summary statistics deviates from model expectations

plotQQunif(m2, testDispersion=FALSE)
```

# Fourth corner model set-up
Creating dataframes for abundance of debris types, environmental variables, and traits of each debris type.
```{r gllvm-set-up, warning=FALSE, message=FALSE, results="hide"}
#Abundance
  #create abundance dataframe with ID as columns and survey events as rows
  abund_long <- site_select %>%
    mutate(survey_event=paste0(Date, Site)) %>% #create column for unique survey event
    group_by(survey_event, ID) %>% 
    summarise(sum=sum(Total)) %>% #total of each item type per survey event
    dplyr::select(survey_event, ID, sum) %>%
  group_by(ID) %>%
  filter(sum>0) %>% #filter IDs that have no values across all surveys
  ungroup()
  
  #convert to data wide
  abund <- spread(abund_long, ID, sum) 
  
  #replace na values with 0
  abund[is.na(abund)] <- 0
  
  #convert survey event to rowname
  length(unique(abund$survey_event)) == nrow(abund) #checking for duplicates
  abund <- column_to_rownames(abund, var="survey_event") #create rownames from survey event

#Environment
  #create survey event and filter zero sum surveys, 
    env <- site_select %>%
    mutate(survey_event=paste0(Date, Site)) %>%
      group_by(survey_event) %>%
      mutate(sum=sum(Total)) %>%
      filter(sum>0) %>%
      dplyr::select(survey_event, Habitat, DistKm) %>%
      distinct()
    
  #convert survey event to rowname
  length(unique(env$survey_event)) == nrow(env) #checking for duplicates
  env <- column_to_rownames(env, var="survey_event") #create rownames from survey event
  #convert Habitat to numeric
  env$Habitat <- as.factor(env$Habitat) #convert character to factor with 2 levels
  env$Habitat <- as.numeric(env$Habitat) #convert factor to numeric

#Traits
  
  #create column of usable IDs to join
  IDs <- abund %>%
    gather(ID, sum) %>%
    dplyr::select(ID) %>%
    unique()
  #create trait df with ID type as row and columns are fishing and dispersiveness
  TR <- site_select %>%
    group_by(ID) %>%
    summarise(sum=sum(Total)) %>%
    left_join(item_code, by="ID") %>%
    dplyr::select(ID, Dispersiveness, Fishing) %>%
    inner_join(IDs, by="ID") %>%
    column_to_rownames(var="ID") #create rownames from ID

  TR$Dispersiveness <- as.factor(TR$Dispersiveness) #convert character to factor with 4 levels
  TR$Fishing <- as.factor(TR$Fishing) #convert character to factor with 2 levels

y <- as.matrix(abund)
X <- scale(as.matrix(env))
TR <- TR
```

```{r wide-format, warning=FALSE, message=FALSE, results="hide"}
# Model without predictors:
gllvm(y, family = "negative.binomial")
# Model where environmental variables, that is, all variables in X are included:
gllvm(y, X, family = "negative.binomial")
# Fourth corner model, where all main effects for environmental variables in X and
# all interactions between variables in X and variables in TR are included:
fit_wide <- gllvm(y, X, TR, family = "negative.binomial")

summary(fit_wide)

plot(fit_wide)
```
```{r long-format, warning=FALSE, message=FALSE, results="hide"}
yX <- reshape(data.frame(cbind(y, X)), direction = "long", varying =
                colnames(y), v.names = "y", timevar = "sp")
#id is the site (1:20), sp is the debris ID, y is the count
TR2 <- data.frame(sp = 1:62, TR)
datalong <- merge(yX, TR2, by = "sp")
```
# Fourth corner model
Coefficient plot shows that there are a few significant variables including Habitat affected by fishing, habitat on its own, and the dispersiveness category of unencumbered drift.  

#Is the unencumbered drift affected by something off-screen?  It looks like it's cutting off the text on the left.  Also, not quite sure how to read the plots.
```{r model-long}
# Model with environmental and trait variables as predictors
fit_allfixed <- gllvm(data=datalong, 
                      formula = y ~ 0 + Habitat + DistKm + 
                        (Habitat + DistKm) : (Dispersiveness + Fishing), 
                      family = "negative.binomial")
coefplot(fit_allfixed)
# plot(fit_allfixed)
```
# Fourth corner heatmap
Here we see that nothing is significantly interacting with distance, however both fishing and unencumbered drift are interacting with habitat significantly.
```{r 4th-corner-heatmap}
coefplot(fit_allfixed, mar = c(4, 11, 1, 1), cex.ylab = 0.8)
fourth <- fit_allfixed$fourth.corner
a <- max( abs(fourth) )
colort <- colorRampPalette(c("blue", "white", "red"))
plot.4th <- levelplot((as.matrix(fourth)), xlab = "Environmental Variables", 
                      ylab = "Debris traits", col.regions = colort(100), cex.lab = 1.3, 
                      at = seq(-a, a, length = 100), scales = list(x = list(rot = 45)))
plot.4th
```


```{r AR1-setup-vignette-example, include=FALSE, results="hide"}
## Number of time points
n <- 6  

## Simulate the process using the MASS
x <- mvrnorm(mu = rep(0,n),
             Sigma = .7 ^ as.matrix(dist(1:n)) )

## Add measurement noise
y <- x + rnorm(n)    

#Specify time variable as a factor
times <- factor(1:n, levels=1:n)
levels(times)

#Assign a grouping variable
group <- factor(rep(1,n))

#combine data into a single df
dat0 <- data.frame(y,times,group)

#fit the model
glmmTMB(y ~ ar1(times + 0 | group), data=dat0) #is group the site?  times would be the date?
```


