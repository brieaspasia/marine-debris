---
title: "stats-tests"
author: "Brie Sherow"
date: "11/11/2020"
output:
  html_document:
    toc: yes
    toc_float: yes
    code_folding: hide
    df_print: paged
  pdf_document:
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r load-libraries, warning=FALSE, message=FALSE, results='hide'}
library(ggplot2) #graphing
library(ggthemes) #graphing templates
library(hrbrthemes) #graphing templates
library(lubridate) #date manipulation
library(forcats) #working with factors
library(tidyverse) #manipulating data
library(knitr) #rmarkdown functions
library(kableExtra) #table layouts
library(magick) #image processing
library(stats) #R stats functions
library(broom) #create summaries from stats objects
library(car) #lm regression
library(MASS) #stats
library(lme4) #glmer function
library(DHARMa) #testing model diagnostics
library(glmmTMB) #fit zero-inflated negative binomial
library(mvabund) #for multivariate stats
library(gllvm) #for multivariate abundance
library(lattice) #fourth corner heatmap

#Tutorial
#https://cran.r-project.org/web/packages/gllvm/vignettes/vignette1.html
```
# Supplementary Materials 
The repository used to store this data can be found at [https://github.com/brieaspasia/marine-debris](https://github.com/brieaspasia/marine-debris)

# Site selection
10 sites at 5 locations were surveyed between 3-5 times each during summer 2019-2020.
```{r create-site-select}
#read in processed survey data
data <- read.csv(file="data/all_data.csv", 
                 header=T, sep=",", 
                 fileEncoding="UTF-8-BOM") #removes special characters

data$Date <- dmy(data$Date) 
data$Notes <- as.character(data$Notes)

#keep only unsw surveys and sites with public piers and >2 survey rounds
site_select <- data %>%
  filter(Team=="UNSW" & 
           Location %in% c("DOUBL","WATSO","PARSL","NEUTR","CLIFT")) %>%
  dplyr::select(-Habitat2, -Fragment, -Whole)

#create a df for survey count per site
sampling_dates <- data %>%
  group_by(Location) %>%
  filter(Team=="UNSW") %>%
  count(Date)

#take away n
sampling_dates$n <- NULL

#create a column for survey count per site
sampling_dates <- sampling_dates %>%
  count(Location) %>%
  arrange(desc(n)) %>%
  rename(NumberSurveys=n)

#join survey count to site select
site_select <- left_join(site_select, sampling_dates, by="Location")

site_select <- site_select %>%
  mutate(survey_event = paste(Date, Site,sep=" "))
```

# Debris Types
CSIRO debris classification system was modified to assign a material type and whether fishing or non-fishing related for each debris item.  Dispersiveness was added as a trait using Tangaroa Blue classification system.
```{r load-item-code}
#load item names
item_code <- read.csv(file="data/CSIRO-code.csv", 
                      header=T, sep=",",
                      fileEncoding="UTF-8-BOM") #removes special characters) 

# #filter out material as some item types may have more than one material type 
# #(ex. fishing items, furniture)
# item_code <- item_code %>%
#   dplyr::select(ID, item_name, Fishing, Dispersiveness) 

site_select <- left_join(site_select, item_code, by="ID", suffix=c("", ".code")) 

#state variables to rename
var_change <- c("Ceramic", "Undetermined", "E-Waste", "Paper", "Construction", "Organic", "Unknown", "Timber", "Foam", "Brick or Cement", "Rubber", "Cloth", "") 

#change Material to character
site_select$Material <- as.character(site_select$Material) 

#set lesser used categories to 'Other'
site_select$Material[site_select$Material %in% var_change] = "Other" 
```

# Site Attributes
Each location included a 25m transect at a pier and an adjacent 25m soft sediment site in an area of boat traffic.  Distance from harbour mouth was calculated using ArcGIS Cost Distance analysis.
```{r load-site-attributes}
#load site coords and distance from harbour mouth
site_attr <- read.csv(file="data/site_attributes.csv", header=T, sep=",", fileEncoding="UTF-8-BOM")

#add distance to harbour mouth to site_select data
site_attr <- site_attr %>%
  dplyr::select(Site, CostDis, Coast) %>% #select only distance to harbour mouth for join
  mutate(DistKm = CostDis/100) #distance to harbour mouth
  
#join distance to mouth to main dataset
site_select <- left_join(site_select, site_attr, by="Site") 
  
```

# GLMM model
Regression isolates the relationship between each independent variable (Habitat and distance) and the dependent variable (Total debris). Total debris count predicted by habitat (pier or sediment) and distance from harbour mouth, both affected by fishing and with location as a random effect.  Offset by the number of surveys at each site to account for differing counts.  Using negative binomial because it is count data with a high number of zeros, and an additional zero inflation because of the zeros.

Debris type is not considered as a random effect because in this instance it is the total debris count that matters rather than the individual debris types.  Accumulation due to repeat surveys is also not considered as the temporal autocorrelation does not indicate that repeat surveys are significant (see below).

# How does the amount of debris differ between pier and soft sediment sites?
```{r glm-hab, warning=FALSE, message=FALSE}
#create df to use for models
mod_hab <- site_select %>%
  group_by(Date, Location, Habitat, Coast, DistKm, Fishing) %>% #relevant variables
  summarise(Total=sum(Total)) 

#GLM negative binomial with two variables and zero inflation
m_hab  <- glmmTMB(Total ~ #Debris count
                 Habitat + #predicted by Habitat (fishing or non-fishing)
                 Coast + #predicted by aspect (fishing or non-fishing)
                 (1|Location/Date), #date random effect
                 # (1|Dispersiveness) + #dispersiveness random effect
                 # ar1(Date + 0 | Location) +
                 # offset(log(NumberSurveys)), #offset by survey count
               # zi = ~1, #zero inflation
               family=nbinom1(), #negative binomial to deal with count data and zeros
               data = mod_hab) 

```
# Summary
  ## AIC estimates out of sample prediction error and thereby relative quality of model
  ## BIC (Bayesian information criterion) is a criterion for model selection among a finite set of models; the model with the lowest BIC is preferred.
  ## logLik is a function of sample size, can be used to compare the fit of different coefficients.  Larger number is better.
  ## Deviance is a measure of error.  lower deviance means better fit to data. The greater the deviance, the worse the model fits compared to the best case (saturated).
  ## df.resid is the sample size minus the number of parameters being measured.
```{r model-summary-hab}
summary(m_hab)
coef(m_hab)
```

```{r simulation-output-hab}
simulationOutput_hab <- simulateResiduals(fittedModel = m_hab, plot=T)
  
plotResiduals(simulationOutput_hab)

# testUniformity(simulationOutput_hab) #tests if the overall distribution conforms to expectations
# testOutliers(simulationOutput_hab) #tests if there are more simulation outliers than expected
testDispersion(simulationOutput_hab) #tests if the simulated dispersion is equal to the observed dispersion
testQuantiles(simulationOutput_hab) #fits a quantile regression or residuals against a predictor (default predicted value), and tests of this conforms to the expected quantile
# testZeroInflation(simulationOutput_fish) #tests if there are more zeros than expected

# #need to set a function for the summary in order to test particular scenarios - still not sure about this one...
# means <- function(x) mean(x) # testing if mean prediction fits
# testGeneric(simulationOutput_hab, summary = means) #test if a generic summary statistics deviates from model expectations

plotQQunif(m_hab, testDispersion=FALSE)
```
```{r glm-fish, warning=FALSE, message=FALSE}
#create df to use for models
mod_fish <- site_select %>%
  group_by(Date, Location, Habitat, Coast, Fishing) %>% #relevant variables
  summarise(Total=sum(Total)) 

#GLM negative binomial with two variables and zero inflation
m_fish  <- glmmTMB(Total ~ #Debris count
                 Fishing*Habitat + #predicted by Habitat (fishing or non-fishing)
                 Fishing*Coast + #predicted by aspect (fishing or non-fishing)
                 (1|Location/Date), #date random effect
                 # (1|Dispersiveness) + #dispersiveness random effect
                 # ar1(Date + 0 | Location) +
                 # offset(log(NumberSurveys)), #offset by survey count
               # zi = ~1, #zero inflation
               family=nbinom2(), #negative binomial to deal with count data and zeros
               data = mod_fish) 

```
```{r model-summary}
summary(m_fish)
coef(m_fish)
```

```{r simulation-output-m2}
simulationOutput_fish <- simulateResiduals(fittedModel = m_fish, plot=T)
  
plotResiduals(simulationOutput_fish)

# testUniformity(simulationOutput_fish) #tests if the overall distribution conforms to expectations
# testOutliers(simulationOutput_fish) #tests if there are more simulation outliers than expected
testDispersion(simulationOutput_fish) #tests if the simulated dispersion is equal to the observed dispersion
testQuantiles(simulationOutput_fish) #fits a quantile regression or residuals against a predictor (default predicted value), and tests of this conforms to the expected quantile
# testZeroInflation(simulationOutput_fish) #tests if there are more zeros than expected

# #need to set a function for the summary in order to test particular scenarios - still not sure about this one...
# means <- function(x) mean(x) # testing if mean prediction fits
# testGeneric(simulationOutput_fish, summary = means) #test if a generic summary statistics deviates from model expectations

plotQQunif(m_fish, testDispersion=FALSE)
```
```{r glm-disp, warning=FALSE, message=FALSE}
#create df to use for models
mod_disp <- site_select %>%
  group_by(Date, Location, Habitat, Coast, Dispersiveness) %>% #relevant variables
  summarise(Total=sum(Total)) 

#GLM negative binomial with two variables and zero inflation
m_disp  <- glmmTMB(Total ~ #Debris count
                 Dispersiveness*Habitat + #predicted by Habitat (dispersiveness)
                 Dispersiveness*Coast + #predicted by aspect (dispersiveness)
                 (1|Location/Date), #site/date random effect
               family=nbinom2(), #negative binomial to deal with count data and zeros
               data = mod_disp) 

```
```{r model-summary-dispersion}
summary(m_disp)
coef(m_disp)
```

```{r simulation-output-m_disp}
simulationOutput_disp <- simulateResiduals(fittedModel = m_disp, plot=T)
  
plotResiduals(simulationOutput_disp)

# testUniformity(simulationOutput_disp) #tests if the overall distribution conforms to expectations
# testOutliers(simulationOutput_disp) #tests if there are more simulation outliers than expected
testDispersion(simulationOutput_disp) #tests if the simulated dispersion is equal to the observed dispersion
testQuantiles(simulationOutput_disp) #fits a quantile regression or residuals against a predictor (default predicted value), and tests of this conforms to the expected quantile
# testZeroInflation(simulationOutput_disp) #tests if there are more zeros than expected

# #need to set a function for the summary in order to test particular scenarios - still not sure about this one...
# means <- function(x) mean(x) # testing if mean prediction fits
# testGeneric(simulationOutput_disp, summary = means) #test if a generic summary statistics deviates from model expectations

plotQQunif(m_disp, testDispersion=FALSE)
```
```{r glm-F2, warning=FALSE, message=FALSE}
#create df to use for models
mod_F2 <- site_select %>%
  filter(ID=="F2") %>% #fishing line
  group_by(Date, Location, Habitat, Coast) %>% #relevant variables
  summarise(Total=sum(Total)) 

#GLM negative binomial with two variables and zero inflation
m_F2  <- glmmTMB(Total ~ #Debris count
                 Habitat + #predicted by Habitat (dispersiveness)
                 Coast + #predicted by distance from mouth (dispersiveness)
                 (1|Location/Date), #site random effect
               family=nbinom2(), #negative binomial to deal with count data and zeros
               data = mod_F2) 

```


```{r model-summary-F2}
summary(m_F2)
coef(m_F2)
```

```{r simulation-output-m_F2}
simulationOutput_F2 <- simulateResiduals(fittedModel = m_F2, plot=T)
  
plotResiduals(simulationOutput_F2)

# testUniformity(simulationOutput_F2) #tests if the overall distribution conforms to expectations
# testOutliers(simulationOutput_F2) #tests if there are more simulation outliers than expected
testDispersion(simulationOutput_F2) #tests if the simulated dispersion is equal to the observed dispersion
testQuantiles(simulationOutput_F2) #fits a quantile regression or residuals against a predictor (default predicted value), and tests of this conforms to the expected quantile
# testZeroInflation(simulationOutput_F2) #tests if there are more zeros than expected

# #need to set a function for the summary in order to test particular scenarios - still not sure about this one...
# means <- function(x) mean(x) # testing if mean prediction fits
# testGeneric(simulationOutput_F2, summary = means) #test if a generic summary statistics deviates from model expectations

plotQQunif(m_F2, testDispersion=FALSE)
```
```{r glm-btl, warning=FALSE, message=FALSE}
#create df to use for models
mod_btl <- site_select %>%
  filter(ID=="G1") %>% #glass bottles
  group_by(Date, Location, Habitat, Coast) %>% #relevant variables
  summarise(Total=sum(Total)) 

#GLM negative binomial with two variables and zero inflation
m_btl  <- glmmTMB(Total ~ #Debris count
                 Habitat + #predicted by Habitat 
                 Coast + #predicted by aspect
                 (1|Location/Date), #site/date random effect
               family=nbinom2(), #negative binomial to deal with count data and zeros
               data = mod_btl) 

```


```{r model-summary-F2}
summary(m_btl)
coef(m_btl)
```

```{r simulation-output-m_btl}
simulationOutput_btl <- simulateResiduals(fittedModel = m_btl, plot=T)
  
plotResiduals(simulationOutput_btl)

# testUniformity(simulationOutput_btl) #tests if the overall distribution conforms to expectations
# testOutliers(simulationOutput_btl) #tests if there are more simulation outliers than expected
testDispersion(simulationOutput_btl) #tests if the simulated dispersion is equal to the observed dispersion
testQuantiles(simulationOutput_btl) #fits a quantile regression or residuals against a predictor (default predicted value), and tests of this conforms to the expected quantile
# testZeroInflation(simulationOutput_btl) #tests if there are more zeros than expected

# #need to set a function for the summary in order to test particular scenarios - still not sure about this one...
# means <- function(x) mean(x) # testing if mean prediction fits
# testGeneric(simulationOutput_btl, summary = means) #test if a generic summary statistics deviates from model expectations

plotQQunif(m_btl, testDispersion=FALSE)
```

```{r glm-hook, warning=FALSE, message=FALSE}
#create df to use for models
mod_hook <- site_select %>%
  filter(ID=="F6") %>% #fishhooks and sinkers
  group_by(Date, Location, Habitat, Coast) %>% #relevant variables
  summarise(Total=sum(Total)) 

#GLM negative binomial with two variables and zero inflation
m_hook  <- glmmTMB(Total ~ #Debris count
                 Habitat + #predicted by Habitat
                 Coast + #predicted by aspect
                 (1|Location/Date), #site/date random effect
               family=nbinom2(), #negative binomial to deal with count data and zeros
               data = mod_hook) 

```


```{r model-summary-hook}
summary(m_hook)
coef(m_hook)
```

```{r simulation-output-m_hook}
simulationOutput_hook <- simulateResiduals(fittedModel = m_hook, plot=T)
  
plotResiduals(simulationOutput_hook)

# testUniformity(simulationOutput_hook) #tests if the overall distribution conforms to expectations
# testOutliers(simulationOutput_hook) #tests if there are more simulation outliers than expected
testDispersion(simulationOutput_hook) #tests if the simulated dispersion is equal to the observed dispersion
testQuantiles(simulationOutput_hook) #fits a quantile regression or residuals against a predictor (default predicted value), and tests of this conforms to the expected quantile
# testZeroInflation(simulationOutput_hook) #tests if there are more zeros than expected

# #need to set a function for the summary in order to test particular scenarios - still not sure about this one...
# means <- function(x) mean(x) # testing if mean prediction fits
# testGeneric(simulationOutput_hook, summary = means) #test if a generic summary statistics deviates from model expectations

plotQQunif(m_hook, testDispersion=FALSE)
```
# Regression coefficients
This is the mean change in the dependent (total debris) variable for each 1 unit change in an independent (habitat and distance) variable.  For instance, each km more from the harbour results in -0.327 less debris items.  Soft sediment sites are likely to have -3.556 less count per item than pier sites.

#I'm not sure how to explain the coefficients on the fishing effect?  And is the intercept the pier sites?  I still get so confused about the intercept and how the model chooses it alphabetically...


# Testing temporal autocorrelation
Despite repeat surveys at each site, temporal autocorrelation testing shows that the effect of repeat surveys is not significant.  This is likely due to the fact that the initial surveys did not actually function as baseline cleans because the sites took many trips to clean and pier sites were never cleared completely.
```{r testing-temporal-autocorrelation, warning=FALSE, message=FALSE, results="hide"}
#creating a column from the model residuals
mod_F2$resid = resid(m_F2) #create a column for each survey event's residuals

#Clifton Gardens - DW = 3.301, p-value = 0.05987
  m_clift <- mod_F2 %>%
    filter(Location=="CLIFT") %>% 
    group_by(Date) %>%
    summarise(sum=sum(resid))
  
  testTemporalAutocorrelation(m_clift$sum, 
                              time =  m_clift$Date) #test resid for temporal autocorrelation

#Double Bay - DW = 2.2347, p-value = 0.8496
  m_doubl <- mod %>%
    filter(Location=="DOUBL") %>%
    group_by(Date) %>%
    summarise(sum=sum(resid))
  
  testTemporalAutocorrelation(m_doubl$sum, 
                              time =  m_doubl$Date) #test resid for temporal autocorrelation

#Neutral Bay - DW = 2.767, p-value = 0.3581
  m_neutr <- mod %>%
    filter(Location=="NEUTR") %>%
    group_by(Date) %>%
    summarise(sum=sum(resid))
  
  testTemporalAutocorrelation(m_neutr$sum, 
                              time =  m_neutr$Date) #test resid for temporal autocorrelation
#Parsley Bay - DW = 1.2492, p-value = 0.4578
    m_parsl <- mod_F2 %>%
    filter(Location=="PARSL") %>%
    group_by(Date) %>%
    summarise(sum=sum(resid))
  
  testTemporalAutocorrelation(m_parsl$sum, 
                              time =  m_parsl$Date) #test resid for temporal autocorrelation
#Watsons Bay - DW = 2.8324, p-value = 0.3741
    m_watso <- mod %>%
    filter(Location=="WATSO") %>%
    group_by(Date) %>%
    summarise(sum=sum(resid))
  
  testTemporalAutocorrelation(m_watso$sum, 
                              time =  m_watso$Date) #test resid for temporal autocorrelation
```

# Fourth corner model set-up
Creating dataframes for abundance of debris types, environmental variables, and traits of each debris type.
```{r gllvm-set-up, warning=FALSE, message=FALSE, results="hide"}
#Abundance
  #create abundance dataframe with ID as columns and survey events as rows
  abund_long <- site_select %>%
    mutate(survey_event=paste0(Date, Site)) %>% #create column for unique survey event
    group_by(survey_event, ID) %>% 
    summarise(sum=sum(Total)) %>% #total of each item type per survey event
    dplyr::select(survey_event, ID, sum) %>%
  group_by(ID) %>%
  filter(sum>0) %>% #filter IDs that have no values across all surveys
  ungroup()
  
  #convert to data wide
  abund <- spread(abund_long, ID, sum) 
  
  #replace na values with 0
  abund[is.na(abund)] <- 0
  
  #convert survey event to rowname
  length(unique(abund$survey_event)) == nrow(abund) #checking for duplicates
  abund <- column_to_rownames(abund, var="survey_event") #create rownames from survey event

#Environment
  #create survey event and filter zero sum surveys, 
    env <- site_select %>%
    mutate(survey_event=paste0(Date, Site)) %>%
      group_by(survey_event) %>%
      mutate(sum=sum(Total)) %>%
      filter(sum>0) %>%
      dplyr::select(survey_event, Habitat, DistKm, Coast) %>%
      distinct()
    
  #convert survey event to rowname
  length(unique(env$survey_event)) == nrow(env) #checking for duplicates
  env <- column_to_rownames(env, var="survey_event") #create rownames from survey event
  #convert Habitat to numeric
  env$Habitat <- as.factor(env$Habitat) #convert character to factor with 2 levels
  env$Habitat <- as.numeric(env$Habitat) #convert factor to numeric
  #convert Coast to numeric
  env$Coast <- as.factor(env$Coast) #convert character to factor with 2 levels
  env$Coast <- as.numeric(env$Coast) #convert factor to numeric

#Traits
  
  #create column of usable IDs to join
  IDs <- abund %>%
    gather(ID, sum) %>%
    dplyr::select(ID) %>%
    unique()
  #create trait df with ID type as row and columns are fishing and dispersiveness
  TR <- site_select %>%
    group_by(ID) %>%
    summarise(sum=sum(Total)) %>%
    left_join(item_code, by="ID") %>%
    dplyr::select(Material, ID, Dispersiveness, Fishing) %>%
    inner_join(IDs, by="ID") %>% #remove items with 0 count
    column_to_rownames(var="ID") #create rownames from ID
  
  #state variables to rename
  var_change <- c("Ceramic", "Undetermined", "E-Waste", "Paper", "Construction", "Organic", "Unknown", "Timber", "Foam", "Brick or Cement", "Rubber", "Cloth", "") 
  
  #change Material to character
  TR$Material <- as.character(TR$Material) 
  
  #set lesser used categories to 'Other'
  TR$Material[TR$Material %in% var_change] = "Other" 

  TR$Dispersiveness <- as.factor(TR$Dispersiveness) #convert character to factor with 4 levels
  TR$Fishing <- as.factor(TR$Fishing) #convert character to factor with 2 levels
  TR$Material <- as.factor(TR$Material) #convert character to factor with 4 levels

y <- as.matrix(abund)
X <- scale(as.matrix(env))
TR <- TR
```

```{r wide-format, warning=FALSE, message=FALSE, results="hide"}
# Model without predictors:
gllvm(y, family = "negative.binomial")
# Model where environmental variables, that is, all variables in X are included:
gllvm(y, X, family = "negative.binomial")
# Fourth corner model, where all main effects for environmental variables in X and
# all interactions between variables in X and variables in TR are included:
fit_wide <- gllvm(y, X, TR, family = "negative.binomial")

summary(fit_wide)

plot(fit_wide)
```
```{r long-format, warning=FALSE, message=FALSE, results="hide"}
yX <- reshape(data.frame(cbind(y, X)), direction = "long", varying =
                colnames(y), v.names = "y", timevar = "sp")
#id is the site (1:20), sp is the debris ID, y is the count
TR2 <- data.frame(sp = 1:62, TR)
datalong <- merge(yX, TR2, by = "sp")
```
# Fourth corner model
Coefficient plot shows that there are a few significant variables including Habitat affected by fishing, habitat on its own, and the dispersiveness category of unencumbered drift.  

#Is the unencumbered drift affected by something off-screen?  It looks like it's cutting off the text on the left.  Also, not quite sure how to read the plots.
```{r model-long}
# Model with environmental and trait variables as predictors
fit_4th <- gllvm(y, X, TR, family = "negative.binomial", num.lv = 2, 
                      formula = y ~ (0 + Habitat + DistKm + Coast) + 
                        (Habitat + DistKm + Coast) : (Material + Dispersiveness + Fishing),
                      seed = 123, row.eff = "random", n.init = 3, jitter.var = 0.01, 
                      randomX = ~ Habitat + DistKm + Coast)


# Model with environmental and trait variables as predictors
fit_allfixed <- gllvm(data=datalong, 
                      formula = y ~ (0 + Habitat + DistKm + Coast) + 
                        (Habitat + DistKm + Coast) : (Material + Dispersiveness + Fishing), 
                      family = "negative.binomial")
coefplot(fit_4th)
coefplot(fit_allfixed)
```
# Fourth corner heatmap
Here we see that nothing is significantly interacting with distance, however both fishing and unencumbered drift are interacting with habitat significantly.
```{r 4th-corner-heatmap}
coefplot(fit_allfixed, mar = c(4, 11, 1, 1), cex.ylab = 0.8)
fourth <- fit_allfixed$fourth.corner
a <- max( abs(fourth) )
colort <- colorRampPalette(c("blue", "white", "red"))
plot.4th <- levelplot((as.matrix(fourth)), xlab = "Environmental Variables", 
                      ylab = "Debris traits", col.regions = colort(100), cex.lab = 1.3, 
                      at = seq(-a, a, length = 100), scales = list(x = list(rot = 45)))
plot.4th

coefplot(fit_4th, mar = c(4, 11, 1, 1), cex.ylab = 0.8)
fourth <- fit_4th$fourth.corner
a <- max( abs(fourth) )
colort <- colorRampPalette(c("blue", "white", "red"))
plot.4th <- levelplot((as.matrix(fourth)), xlab = "Environmental Variables", 
                      ylab = "Debris traits", col.regions = colort(100), cex.lab = 1.3, 
                      at = seq(-a, a, length = 100), scales = list(x = list(rot = 45)))
plot.4th
```


```{r glm-material, warning=FALSE, message=FALSE}

#create df to use for models
mod <- mat_select %>%
  dplyr::filter(Material=="Glass") %>%
  group_by(Date, Location, Habitat, DistKm, NumberSurveys,
           Fishing) %>% #relevant variables
  summarise(Total=sum(Total)) 

#GLM negative binomial with two variables
m_glass  <- glmmTMB(Total ~ #Debris count
                 Fishing*Habitat + #predicted by Habitat (fishing or non-fishing)
                 Fishing*DistKm + #predicted by distance from mouth (fishing or non-fishing)
                 (1|Location/Date), #site random effect
                 # (1|Dispersiveness) + #dispersiveness random effect
                 # ar1(Date + 0 | Location) +
                 # offset(log(NumberSurveys)), #offset by survey count
               # zi = ~1, #zero inflation
               family=nbinom2(), #negative binomial to deal with count data and zeros
               data = mod) 
```

